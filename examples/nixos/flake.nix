# Flake configuration for Jetson AGX Orin Developer Kit
#
# Available configurations (pick one):
#
#   nixos-rebuild switch --flake .#nixos                Base system (DHCP)
#   nixos-rebuild switch --flake .#nixos-static-ip      Static IP on eth0
#   nixos-rebuild switch --flake .#nixos-perf           Base + performance tuning (MAXN, clocks, hugepages)
#   nixos-rebuild switch --flake .#nixos-llama-cpp      Base + perf + llama.cpp server
#   nixos-rebuild switch --flake .#nixos-tabby-api      Base + perf + TabbyAPI server
#   nixos-rebuild switch --flake .#nixos-telemetry      Base + Grafana/Prometheus GPU telemetry

{
  description = "NixOS configuration for Jetson AGX Orin";

  inputs = {
    # matches the autogenerated configuration.nix on device
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-25.11";

    # Jetpack NixOS - NVIDIA Jetson support
    jetpack.url = "github:anduril/jetpack-nixos/master";
    jetpack.inputs.nixpkgs.follows = "nixpkgs";
  };

  outputs = { self, nixpkgs, jetpack, ... }:
    let
      # Shared base modules â€” every config gets these
      baseModules = [
        jetpack.nixosModules.default
        ./configuration.nix
      ];
    in
    {
      # -- Base: plain DHCP networking --
      nixosConfigurations.nixos = nixpkgs.lib.nixosSystem {
        modules = baseModules;
      };

      # -- Static IP on eth0 --
      nixosConfigurations.nixos-static-ip = nixpkgs.lib.nixosSystem {
        modules = baseModules ++ [
          ({ ... }: {
            networking.useDHCP = false;
            networking.interfaces.eth0.ipv4.addresses = [{
              address = "192.168.0.131";
              prefixLength = 24;
            }];
          })
        ];
      };

      # -- Performance tuning (MAXN, locked clocks, hugepages, zram) --
      nixosConfigurations.nixos-perf = nixpkgs.lib.nixosSystem {
        modules = baseModules ++ [
          ./modules/performance.nix
          ({ ... }: {
            services.orin-perf.enable = true;
          })
        ];
      };

      # -- llama.cpp server + performance tuning --
      nixosConfigurations.nixos-llama-cpp = nixpkgs.lib.nixosSystem {
        modules = baseModules ++ [
          ./modules/performance.nix
          ./modules/llama-cpp-server.nix
          ({ ... }: {
            services.orin-perf.enable = true;
            services.llama-cpp-server = {
              enable = true;
              model = "/models/Qwen3-Coder-Next-Q4_K_M.gguf";
              nGpuLayers = 99;
              contextSize = 4096;
              threads = 8;
              batchSize = 512;
              flashAttn = true;
              useMmap = true;
              extraArgs = [
                "--cont-batching"
                "--parallel" "2"
              ];
            };
          })
        ];
      };

      # -- TabbyAPI server + performance tuning --
      nixosConfigurations.nixos-tabby-api = nixpkgs.lib.nixosSystem {
        modules = baseModules ++ [
          ./modules/performance.nix
          ./modules/tabby-api.nix
          ({ ... }: {
            services.orin-perf.enable = true;
            services.tabby-api = {
              enable = true;
              modelDir = "/models";
              modelName = "Qwen3-Coder-Next-Q4_K_M.gguf";
              maxSeqLen = 4096;
              cacheMode = "Q4";
            };
          })
        ];
      };

      # -- Telemetry (Grafana + Prometheus + tegrastats) --
      nixosConfigurations.nixos-telemetry = nixpkgs.lib.nixosSystem {
        modules = baseModules ++ [
          ./modules/telemetry.nix
          ({ ... }: {
            services.jetson-telemetry = {
              enable = true;
              enableTegrastats = true;
              enableNodeExporter = true;
              enableOpenTelemetry = true;
            };
          })
        ];
      };

      # -- Docker benchmarking (Docker + NVIDIA Container Toolkit) --
      # Enables Docker with GPU passthrough for running vLLM, MLC LLM,
      # and other containerized inference engines.
      #
      #   sudo nixos-rebuild switch --flake .#nixos-docker-bench
      #   docker run --runtime nvidia -e NVIDIA_VISIBLE_DEVICES=all ubuntu nvidia-smi
      #
      nixosConfigurations.nixos-docker-bench = nixpkgs.lib.nixosSystem {
        modules = baseModules ++ [
          ./modules/performance.nix
          ./modules/docker-nvidia.nix
          ({ ... }: {
            services.orin-perf.enable = true;
            services.docker-nvidia = {
              enable = true;
              users = [ "agent" "spencer" ];
            };
          })
        ];
      };
    };
}