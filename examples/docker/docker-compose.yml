version: '3.8'

services:
  tensorrt-llm:
    build:
      context: .
      dockerfile: Dockerfile
    image: jetpack-tensorrt-llm:latest
    container_name: tensorrt-llm
    
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
    
    ports:
      - "8000:8000"  # API server
    
    volumes:
      # Persist models and TensorRT engines
      - ./models:/workspace/models
      - ./engines:/workspace/engines
      - ./scripts:/workspace/scripts
    
    stdin_open: true
    tty: true
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    restart: unless-stopped
    
    shm_size: '8gb'  # Important for large models
    
    working_dir: /workspace
