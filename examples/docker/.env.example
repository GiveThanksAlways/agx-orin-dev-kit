# TensorRT-LLM Configuration
# Copy to .env and customize

# GPU Configuration
CUDA_VISIBLE_DEVICES=0
NVIDIA_VISIBLE_DEVICES=all

# Model paths (inside container)
MODEL_DIR=/workspace/models
ENGINE_DIR=/workspace/engines

# API Server
API_HOST=0.0.0.0
API_PORT=8000

# TensorRT Build Options
TRTLLM_MAX_BATCH_SIZE=8
TRTLLM_MAX_INPUT_LEN=2048
TRTLLM_MAX_OUTPUT_LEN=512
TRTLLM_DTYPE=float16  # Options: float16, float32, int8, int4

# HuggingFace
HF_TOKEN=  # Optional: for private models
HF_HOME=/workspace/.cache/huggingface
