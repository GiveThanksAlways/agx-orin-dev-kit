# TensorRT-LLM on Jetson ðŸš€

**Ultra-fast LLM inference using NVIDIA TensorRT. 10-100x faster than CPU, 2-5x faster than standard GPU.**

## âš¡ Super Quick Start

```bash
./quickstart.sh                          # Setup (5 min)
docker exec -it tensorrt-llm bash        # Enter container
./scripts/convert-and-run.sh qwen3       # Download + run (10 min)
```

**That's it!** Model running with TensorRT optimizations.

---

## Why TensorRT-LLM?

- âš¡ **Fastest inference** - Custom kernels, INT8/INT4 quantization, operator fusion
- ðŸŽ¯ **Optimized for Jetson** - Native CUDA acceleration, efficient memory
- ðŸ”¥ **Production ready** - NVIDIA's official stack, battle-tested

## Quick Start (Detailed)

```bash
# 1. Start container
docker-compose up -d
docker exec -it tensorrt-llm bash

# 2. Download model
huggingface-cli download unsloth/Qwen3-Coder-Next-GGUF --local-dir /workspace/models/qwen3

# 3. Convert & run (inside container)
./scripts/convert-and-run.sh qwen3
```

Done! Model running with TensorRT optimizations.

## Prerequisites

- Docker + NVIDIA runtime: `docker run --rm --gpus all nvidia/cuda:12.4.1-base nvidia-smi`
- 20GB+ disk space
- 8GB+ VRAM (adjust model size accordingly)

## Step-by-Step

### 1. Setup
```bash
cd /path/to/jetpack-nixos/examples/docker
docker-compose build  # Pull TensorRT-LLM NGC image
docker-compose up -d
```

### 2. Download Model (HuggingFace)
```bash
# Enter container
docker exec -it tensorrt-llm bash

# Download Qwen3-Coder (example)
huggingface-cli download unsloth/Qwen3-Coder-Next-GGUF \
  --include "*.gguf" \
  --local-dir /workspace/models/qwen3
```

### 3. Convert to TensorRT Engine

TensorRT engines are model-specific compiled binaries optimized for your GPU:

```bash
# Inside container
trtllm-build \
  --checkpoint_dir /workspace/models/qwen3 \
  --output_dir /workspace/engines/qwen3 \
  --gemm_plugin auto \
  --max_batch_size 8 \
  --max_input_len 2048 \
  --max_output_len 512
```

This creates `.engine` files (~5-10 min, one-time setup).

### 4. Run Inference

```bash
# Python API
python /workspace/run-model.py \
  --engine /workspace/engines/qwen3 \
  --prompt "Write a hello world program"

# Interactive
python /workspace/run-model.py \
  --engine /workspace/engines/qwen3 \
  --interactive
```

## Performance Comparison

| Method | Tokens/sec | Latency |
|--------|-----------|---------|
| CPU (llama.cpp) | ~5 | High |
| GPU (PyTorch) | ~20-30 | Medium |
| **TensorRT-LLM** | **50-150** | **Low** |

*Jetson Orin AGX, 7B model, INT8 quantization*

## Models & VRAM

| Model Size | VRAM | Speed |
|------------|------|-------|
| 7B (FP16) | ~14GB | Fast |
| 7B (INT8) | ~7GB | Faster |
| 7B (INT4) | ~4GB | Fastest |
| 14B (INT8) | ~14GB | Medium |
| 32B (INT4) | ~18GB | Slow |

## Common Commands

```bash
# Enter container
docker exec -it tensorrt-llm bash

# Check GPU
nvidia-smi

# Interactive mode
python /workspace/run-model.py --engine /workspace/engines/qwen3 --interactive

# API Server (OpenAI-compatible)
python /workspace/api-server.py --engine /workspace/engines/qwen3 --port 8000
# Test: curl http://localhost:8000/v1/completions -d '{"prompt":"Hello","max_tokens":50}'

# Benchmark performance
./scripts/benchmark.sh qwen3 10

# List models/engines
ls /workspace/models
ls /workspace/engines

# Rebuild engine (after model update)
rm -rf /workspace/engines/model-name
./scripts/convert-and-run.sh model-name

# Monitor performance
watch nvidia-smi

# Stop
docker-compose down
```

## Troubleshooting

**OOM during conversion:** Reduce `--max_batch_size` or use INT4/INT8 quantization  
**Slow inference:** Check `nvidia-smi` for other GPU processes  
**Engine build fails:** Verify model format (HuggingFace transformers or GGUF)  
**Import errors:** Container might still be starting, wait 30s

## Advanced: Custom Models

Any HuggingFace model works:

```bash
# Meta Llama 3
huggingface-cli download meta-llama/Llama-3.1-70B-Instruct \
  --local-dir /workspace/models/llama3

# Convert
trtllm-build \
  --checkpoint_dir /workspace/models/llama3 \
  --output_dir /workspace/engines/llama3 \
  ...
```

## Scripts

- `quickstart.sh` - One-command setup (downloads NGC image, starts container)
- `convert-and-run.sh` - Auto-converts model to TensorRT engine + runs inference
- `run-model.py` - Python inference runner (CLI and interactive mode)
- `api-server.py` - OpenAI-compatible REST API server
- `benchmark.sh` - Performance benchmarking tool
- `download-model.sh` - Helper to fetch models from HuggingFace

## Use Cases

**Development/Testing:**
```bash
docker exec -it tensorrt-llm bash
python run-model.py --engine engines/qwen3 --interactive
```

**Production API:**
```bash
docker exec -d tensorrt-llm python api-server.py --engine engines/qwen3
# Use OpenAI SDK to connect to localhost:8000
```

**Benchmarking:**
```bash
docker exec tensorrt-llm ./scripts/benchmark.sh qwen3 20
```

## Links

- [TensorRT-LLM GitHub](https://github.com/NVIDIA/TensorRT-LLM)
- [NGC Containers](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tensorrt-llm)
- [Model Zoo](https://huggingface.co/models?other=tensorrt-llm)
